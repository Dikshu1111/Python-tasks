{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0a066e-99b0-4eeb-8c2e-ed78f4bf8560",
   "metadata": {},
   "source": [
    "TASK 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff50a8-63d9-4dc4-bb79-96426f0cc5d4",
   "metadata": {},
   "source": [
    "1) Scrape all product names and prices from the first two pages of \"Books to Scrape\" (http://books.toscrape.com/). Handle simple pagination and structure the output as a list of dictionaries with 'title' and 'price'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3048ff-4a64-45f0-8eda-c3f368ad5652",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m      4\u001b[39m base_url = \u001b[33m\"\u001b[39m\u001b[33mhttp://books.toscrape.com/catalogue/page-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.html\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "all_books = []\n",
    "\n",
    "for page in range(1, 3):  # first 2 pages\n",
    "    url = base_url.format(page)\n",
    "    response = requests.get(url)\n",
    "\n",
    "    with open(f'page-{page}.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "    for book in books:\n",
    "        title = book.h3.a['title']\n",
    "        price = book.find('p', class_='price_color').text\n",
    "        all_books.append({'title': title, 'price': price})\n",
    "\n",
    "\n",
    "print(all_books)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be800d3c-6408-4d1b-85ba-6ea16e1a586b",
   "metadata": {},
   "source": [
    "2) Extract the current weather descriptions (like ‘clear’, ‘cloudy’) and temperatures for at least five cities from a public weather site (such as https://www.weather.com or https://wttr.in). Present your data in a tabular format (city, description, temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943124ed-dc17-43fa-a8ad-2c828d42e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      City Description Temperature\n",
      "0   Mumbai        Haze         89°\n",
      "1     Pune        Haze         78°\n",
      "2   Nashik        Mist         81°\n",
      "3   Nagpur      Cloudy         79°\n",
      "4  Jalgaon         Fog         78°\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# City names and their weather.com location codes\n",
    "cities = {\n",
    "    'Mumbai': 'INXX0096:1:IN',\n",
    "    'Pune': 'INXX0113:1:IN',\n",
    "    'Nashik': 'INXX0121:1:IN',\n",
    "    'Nagpur': 'INXX0114:1:IN',\n",
    "    'Jalgaon': 'INXX0093:1:IN'\n",
    "}\n",
    "\n",
    "base_url = \"https://weather.com/weather/today/l/\"\n",
    "\n",
    "\n",
    "\n",
    "weather_list = []\n",
    "\n",
    "for city, code in cities.items():\n",
    "    url = base_url + code\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract temperature\n",
    "    temp_tag = soup.find('span', attrs={'data-testid': 'TemperatureValue'})\n",
    "    temperature = temp_tag.text if temp_tag else 'N/A'\n",
    "\n",
    "    # Extract description\n",
    "    desc_tag = soup.find('div', attrs={'data-testid': 'wxPhrase'})\n",
    "    description = desc_tag.text if desc_tag else 'N/A'\n",
    "\n",
    "    weather_list.append({\n",
    "        'City': city,\n",
    "        'Description': description,\n",
    "        'Temperature': temperature\n",
    "    })\n",
    "\n",
    "# Create DataFrame \n",
    "df = pd.DataFrame(weather_list)\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b023d07-dd1a-4142-bfbe-19451df27367",
   "metadata": {},
   "source": [
    "3) From the “Real Python Fake Jobs” board (https://realpython.github.io/fake-jobs/), gather all job titles, companies, and locations listed on the first three pages. Save the results as a CSV file. Be sure to loop through the pagination and properly parse the HTML for structured data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c3030a-dc7f-4c9a-841c-7cdc0eb5242b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 jobs to fake_jobs.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://realpython.github.io/fake-jobs\"\n",
    "jobs = []\n",
    "\n",
    "# Loop through pages 1 to 3\n",
    "for page in range(1, 4):\n",
    "    if page == 1:\n",
    "        url = base_url + \"/\"\n",
    "    else:\n",
    "        url = f\"{base_url}/page/{page}/\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find all job listings\n",
    "    job_cards = soup.find_all(\"div\", class_=\"card-content\")\n",
    "    \n",
    "    for job in job_cards:\n",
    "        title = job.find(\"h2\", class_=\"title\").text.strip()\n",
    "        company = job.find(\"h3\", class_=\"company\").text.strip()\n",
    "        location = job.find(\"p\", class_=\"location\").text.strip()\n",
    "        \n",
    "        jobs.append({\n",
    "            \"Job Title\": title,\n",
    "            \"Company\": company,\n",
    "            \"Location\": location\n",
    "        })\n",
    "\n",
    "# Save results as CSV\n",
    "df = pd.DataFrame(jobs)\n",
    "df.to_csv(\"fake_jobs.csv\", index=False)\n",
    "\n",
    "print(\"Saved\", len(jobs), \"jobs to fake_jobs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54200471-6845-4929-a505-e136b6f6f2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
